{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPsguCk26qEpX4vSs9OcC/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhyha/CodeMind_RAG/blob/main/RAG/CodeMind_RAG_V5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 레포지토리에서 .env와 requirements.txt content에 업로드하여 붙여넣기\n",
        "# -U update   -q quiet  -r read\n",
        "# env.txt 파일 -> .env 로 변경 후  경로 붙여넣기"
      ],
      "metadata": {
        "id": "X3P9YIkz7kaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q -r /content/requirement.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7CRsSb17Xux",
        "outputId": "57869f88-cc72-453f-e343-6719042bd900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.5/311.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.5/222.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.3/291.3 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.2.2 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.2.2 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.2.2 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from dotenv import load_dotenv\n",
        "# import os\n",
        "\n",
        "# # Specify the path to your .env file\n",
        "# env_path = '/content/.env'\n",
        "\n",
        "# # Load the .env file from the specified path\n",
        "# load_dotenv(dotenv_path=env_path)"
      ],
      "metadata": {
        "id": "wEeF9ty7jT5i",
        "outputId": "148abe3e-23bf-4eb5-8964-455f1a4bbb59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eJHnj5W0Mx-"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain_community.document_loaders.hugging_face_dataset import (\n",
        "    HuggingFaceDatasetLoader,\n",
        ")\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.llms import HuggingFaceHub\n",
        "import os\n",
        "\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit"
      ],
      "metadata": {
        "id": "HKHNafjC1eP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# import faiss\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain import hub\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# # 환경변수 설정\n",
        "# os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'your token'\n",
        "# os.environ['OPENAI_API_KEY'] = 'your token'\n",
        "\n",
        "# # Langsmith\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = \"your token\"\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = \"streamlit_v2\"\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "\n",
        "# 환경변수 설정\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__b34e03adb0cb40e497e967360e32e029\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"streamlit_v2\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "\n",
        "# 데이터 및 컴포넌트 로드를 위한 함수\n",
        "@st.cache_data\n",
        "def load_components():\n",
        "    # 데이터 로드\n",
        "    dataset_name = \"greengerong/leetcode\"\n",
        "    loader = HuggingFaceDatasetLoader(dataset_name, page_content_column=\"content\")\n",
        "    docs = loader.load()\n",
        "\n",
        "    # RAG 성능 검증, 100번 이상의 번호를 넣어보자\n",
        "    limited_docs = docs[:100]\n",
        "    # Initialize the text splitter with specified chunk size and overlap\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=100)\n",
        "    # Apply the splitter to the reduced list of documents\n",
        "    docs = text_splitter.split_documents(limited_docs)\n",
        "\n",
        "    # 'title', 'slug', 'page_content' 결합\n",
        "    for doc in docs:\n",
        "        title = doc.metadata.get('title', 'No Title') if 'title' in doc.metadata else 'No Title'\n",
        "        slug = doc.metadata.get('slug', 'No Slug') if 'slug' in doc.metadata else 'No Slug'\n",
        "        id = doc.metadata.get('id', 'No ID') if 'id' in doc.metadata else 'No ID'\n",
        "        page_content = doc.page_content\n",
        "        doc.page_content = f\"Title: {id}. {title}\\n\\nSlug: {id}. {slug}\\n\\n{page_content}\"  # 'page_content' 속성 수정\n",
        "\n",
        "    # 문서 분할\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=100)\n",
        "    split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "    # 벡터 스토어 및 검색 엔진 설정\n",
        "    vectorstore = FAISS.from_documents(documents=split_docs, embedding=HuggingFaceBgeEmbeddings())\n",
        "    bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
        "    bm25_retriever.k = 1\n",
        "    faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "    ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0, 1])\n",
        "\n",
        "    # return ensemble_retriever, split_docs\n",
        "    return faiss_retriever, split_docs\n",
        "\n",
        "# ensemble_retriever, split_docs = load_components()\n",
        "faiss_retriever, split_docs = load_components()\n",
        "\n",
        "############### llm 모델과 prompt 정의\n",
        "\n",
        "class StreamCallback(BaseCallbackHandler):\n",
        "    def on_llm_new_token(self, token: str, **kwargs):\n",
        "        print(token, end=\"\", flush=True)\n",
        "\n",
        "# LLM 설정\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True, callbacks=[StreamCallback()])\n",
        "\n",
        "prompt = hub.pull(\"nhyha/rag-prompt\")\n",
        "\n",
        "\n",
        "################ 입력받아 적절한 형태의 llm모델의 input으로 변환\n",
        "\n",
        "\n",
        "# 사용자 인터페이스\n",
        "def format_docs(retrieved_docs, language):\n",
        "    formatted_docs = []\n",
        "    for doc in retrieved_docs:\n",
        "        language_content = doc.metadata.get(language.lower(), f'No {language} code available')\n",
        "        title = doc.metadata.get('title', 'No Title') if 'title' in doc.metadata else 'No Title'\n",
        "        slug = doc.metadata.get('slug', 'No Slug') if 'slug' in doc.metadata else 'No Slug'\n",
        "        id = doc.metadata.get('id', 'No ID') if 'id' in doc.metadata else 'No ID'\n",
        "        # formatted_doc = f\"{doc.page_content}\\n\\nTitle: {id}. {title}\\n\\nSlug: {id}. {slug}\\n\\n{language}:{language_content}\"\n",
        "        formatted_doc = f\"Title: {id}. {title}\\n\\n{language}:{language_content}\"\n",
        "        formatted_docs.append(formatted_doc)\n",
        "    return \"\\n\\n\".join(formatted_docs)\n",
        "\n",
        "\n",
        "st.title(\"CodeMind_RAG\")\n",
        "st.markdown('''🏆**CodeMind 선서**🏆<br>\n",
        "<span style=\"color: black; font-size: 14px;\">저는 다음과 같은 내용을 지킬 것을 선언합니다.<br><br>\n",
        "<span style=\"color: black; font-size: 20px;\">#1</span>  저는 결코 제 데이터 베이스에 존재하지 않는 내용에 대해서<br>\n",
        "  **사실처럼 꾸며내거나 거짓 답변**하지 않겠습니다.<br><br>\n",
        "<span style=\"color: black; font-size: 20px;\">#2</span>  저는 현재 제가 거짓으로 답변하지 않는다는 것을 증명하고자<br>\n",
        "  **LeetCode 1 ~ 100번 문제만** DB에 저장되어 있는 상태입니다.<br><br>\n",
        "  **따라서 101번 이상의 문제를 물어보신다면 현재는 모른다고 대답하는 게 정상입니다!**<br><br>\n",
        "  물론, **101번 그 이상까지 혹은 또 다른 사이트의 문제까지도**<br>\n",
        "  데이터 베이스에 얼마든지 내용을 **추가**하실 수 있습니다.<br><br>\n",
        "<span style=\"color: black; font-size: 20px;\">#3</span>  제가 답변드리는 내용은 **수 많은 개발자분들께서 추천**한<br>\n",
        "  **solution**을 토대로 선별하여 요약 정리해드리고 있습니다. 활용에 참고해주세요! 감사합니다🤗</span>\n",
        "  ''', unsafe_allow_html=True)\n",
        "# query = st.text_input(\"질문을 입력하세요:\", key=\"query_input\")\n",
        "query = st.text_area(\"질문을 입력하세요:\", key=\"query_input\")\n",
        "type_select = st.selectbox(\"Select the programming language:\", [\"Python\", \"C++\", \"Java\", \"JavaScript\"], key=\"language_select\")\n",
        "\n",
        "if query:\n",
        "    # 문서 검색 실행\n",
        "    # retrieved_docs = ensemble_retriever.get_relevant_documents(query)\n",
        "    retrieved_docs = faiss_retriever.get_relevant_documents(query)\n",
        "\n",
        "    # 문서 형식 정의\n",
        "    if retrieved_docs:\n",
        "        formatted_docs = format_docs(retrieved_docs, type_select)\n",
        "    else:\n",
        "        formatted_docs = \"검색된 문서가 없습니다.\"\n",
        "\n",
        "    # Correctly set the input for RunnablePassthrough objects\n",
        "    # Assuming there's a method to set input or modify the chain to accept inputs directly\n",
        "    context_runnable = RunnablePassthrough.assign(context=lambda x: formatted_docs)\n",
        "    question_runnable = RunnablePassthrough.assign(question=lambda x: query)\n",
        "\n",
        "    # Define the chain with the corrected usage of RunnablePassthrough objects\n",
        "    rag_chain = (\n",
        "        context_runnable\n",
        "        | question_runnable\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    # Invoke the chain\n",
        "    response = rag_chain.invoke({\"context\": formatted_docs, \"question\": query})\n",
        "    st.markdown(\"\\n\\n💡**CodeMindChat**\\n\\n\")\n",
        "    st.write(response)\n",
        "\n",
        "\n",
        "\n",
        "# =====================================\n",
        "\n",
        "# # def format_docs(docs):\n",
        "# #     formatted_docs = []\n",
        "# #     for doc in docs:\n",
        "# #         title = doc.metadata.get('title', 'No Title') if 'title' in doc.metadata else 'No Title'\n",
        "# #         slug = doc.metadata.get('slug', 'No Slug')  if 'slug' in doc.metadata else 'No Slug'\n",
        "# #         id = doc.metadata.get('id', 'No ID') if 'id' in doc.metadata else 'No ID'\n",
        "# #         py = doc.metadata.get ('python','No Python') if 'python' in doc.metadata else 'No Python'\n",
        "# #         cpp = doc.metadata.get ('cpp','No Cpp') if 'cpp' in doc.metadata else 'No Cpp'\n",
        "# #         java = doc.metadata.get ('java','No Java') if 'java' in doc.metadata else 'No Java'\n",
        "# #         javascript = doc.metadata.get ('javascript','No Javascript') if 'javascript' in doc.metadata else 'No Javascript'\n",
        "# #         page_content = doc.page_content\n",
        "# #         formatted_doc = f\"Title: {id}. {title}\\n\\nSlug: {id}. {slug}\\n\\n{page_content}\"\n",
        "# #         formatted_docs.append(formatted_doc)\n",
        "# #     return \"\\n\\n\".join(formatted_docs)\n",
        "\n",
        "# def format_docs(docs):\n",
        "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# # Streamlit 인터페이스\n",
        "# st.title(\"문서 검색 및 응답 생성\")\n",
        "# query = st.text_input(\"질문을 입력하세요:\")\n",
        "\n",
        "# if query:\n",
        "#     rag_chain = (\n",
        "#         {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "#         | prompt\n",
        "#         | llm\n",
        "#         | StrOutputParser()\n",
        "#     )\n",
        "#     response = rag_chain.invoke(query)\n",
        "#     st.write(\"Response:\\n\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCMZdFPDH3kR",
        "outputId": "d52f6b3f-617b-4caf-81cb-a0dfe69aaeb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2f9zvJxKCwgBCOHmyWDKw9FSRwQ_ufQqGCV7pvwgcKbA8zfQ"
      ],
      "metadata": {
        "id": "PEXjpKdB0wkK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb29fef-ca39-4ece-d449-99c8f5629a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok http --domain=intimate-neutral-skink.ngrok-free.app"
      ],
      "metadata": {
        "id": "SOqrVDKH0w-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a59d13-ab80-4cbc-c840-26e1c4b68719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http - start an HTTP tunnel\n",
            "\n",
            "USAGE:\n",
            "  ngrok http [address:port | port] [flags]\n",
            "\n",
            "AUTHOR:\n",
            "  ngrok - <support@ngrok.com>\n",
            "\n",
            "COMMANDS: \n",
            "  config          update or migrate ngrok's configuration file\n",
            "  http            start an HTTP tunnel\n",
            "  tcp             start a TCP tunnel\n",
            "  tunnel          start a tunnel for use with a tunnel-group backend\n",
            "\n",
            "EXAMPLES: \n",
            "  ngrok http 80                                                 # secure public URL for port 80 web server\n",
            "  ngrok http --domain baz.ngrok.dev 8080                        # port 8080 available at baz.ngrok.dev\n",
            "  ngrok tcp 22                                                  # tunnel arbitrary TCP traffic to port 22\n",
            "  ngrok http 80 --oauth=google --oauth-allow-email=foo@foo.com  # secure your app with oauth\n",
            "\n",
            "Paid Features: \n",
            "  ngrok http 80 --domain mydomain.com                           # run ngrok with your own custom domain\n",
            "  ngrok http 80 --allow-cidr 2600:8c00::a03c:91ee:fe69:9695/32  # run ngrok with IP policy restrictions\n",
            "  Upgrade your account at https://dashboard.ngrok.com/billing/subscription to access paid features\n",
            "\n",
            "Upgrade your account at https://dashboard.ngrok.com/billing/subscription to access paid features\n",
            "\n",
            "Flags:\n",
            "  -h, --help      help for ngrok\n",
            "\n",
            "Use \"ngrok [command] --help\" for more information about a command.\n",
            "\n",
            "ERROR:  You must specify a single argument: a port or address to tunnel to.\n",
            "ERROR:  You specified 0 arguments: []\n",
            "ERROR:  For example, to expose port 80, run 'ngrok http 80'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# 포트 8501에 대한 ngrok 터널을 설정합니다\n",
        "http_tunnel = ngrok.connect(addr=\"8501\", proto=\"http\", bind_tls=True)\n",
        "print('Streamlit이 다음 URL에서 실행 중입니다:', http_tunnel.public_url)\n",
        "\n",
        "!streamlit run app.py &>/dev/null&"
      ],
      "metadata": {
        "id": "8W38LAtC0z39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8043ea0-87a0-4ffe-de75-4d43bfa159c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit이 다음 URL에서 실행 중입니다: https://1a41-35-187-250-224.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w_EWe4FSyzrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fRS35QWrDbxK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jIz5oyETje8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=============================="
      ],
      "metadata": {
        "id": "dzJ45ozb9aAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "p2CotEgh_kA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever gpt-3.5-turbo"
      ],
      "metadata": {
        "id": "lqHSvlHR1Vr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain import hub\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# # 환경변수 설정\n",
        "# os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'your token'\n",
        "# os.environ['OPENAI_API_KEY'] = 'your token'\n",
        "\n",
        "# # Langsmith\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = \"your token\"\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = \"streamlit_v2\"\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "\n",
        "# 데이터 로드\n",
        "dataset_name = \"greengerong/leetcode\"\n",
        "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column=\"content\")\n",
        "docs = loader.load()\n",
        "\n",
        "\n",
        "\n",
        "# 'title', 'slug', 'page_content' 결합\n",
        "for doc in docs:\n",
        "    title = doc.metadata.get('title', 'No Title') if 'title' in doc.metadata else 'No Title'\n",
        "    slug = doc.metadata.get('slug', 'No Slug') if 'slug' in doc.metadata else 'No Slug'\n",
        "    id = doc.metadata.get('id', 'No ID') if 'id' in doc.metadata else 'No ID'\n",
        "    page_content = doc.page_content\n",
        "    doc.page_content = f\"Title: {id}. {title}\\n\\nSlug: {id}. {slug}\\n\\n{page_content}\"  # 'page_content' 속성 수정\n",
        "\n",
        "# 변경된 page_content를 반영하기 위해 문서를 다시 분할\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=100)\n",
        "split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "# 변경된 문서 내용으로 벡터 스토어 및 검색 엔진 재설정\n",
        "vectorstore = FAISS.from_documents(documents=split_docs, embedding=HuggingFaceBgeEmbeddings())\n",
        "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
        "bm25_retriever.k = 6\n",
        "faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.1, 0.9])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class StreamCallback(BaseCallbackHandler):\n",
        "    def on_llm_new_token(self, token: str, **kwargs):\n",
        "        print(token, end=\"\", flush=True)\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        "    streaming=True,\n",
        "    callbacks=[StreamCallback()],\n",
        ")\n",
        "\n",
        "def format_docs(docs):\n",
        "    # 검색한 문서 결과를 하나의 문단으로 합쳐줍니다.\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "prompt = hub.pull(\"nhyha/rag-prompt\")\n"
      ],
      "metadata": {
        "id": "21Zzji601cV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 단계 8: 체인 생성(Create Chain)\n",
        "rag_chain = (\n",
        "    {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# question = \"Please write example code to solve the problem of minimum-window-substring.\"\n",
        "# question = \"Please write example code to solve the problem of Integer to Roman\"\n",
        "# question = \"Please write example code to solve the problem of LeetCode number 12.\"\n",
        "\n",
        "# question = \"Please write example code to solve the problem of LeetCode number #28.\"\n",
        "question = \"Please write example code to solve the problem of Find the Index of the First Occurrence in a String\"\n",
        "\n",
        "response = rag_chain.invoke(question)\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"Dataset Path: {dataset_name}\")\n",
        "print(f\"문서의 수: {len(split_docs)}\")\n",
        "print(\"===\" * 20)\n",
        "print(f\"[HUMAN]\\n{question}\\n\")\n",
        "print(f\"[AI]\\n{response}\")"
      ],
      "metadata": {
        "id": "fGPmkXeJE32y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_retriever.invoke(\"Please write example code to solve the problem of minimum-window-substring.\")\n",
        "# slug : minimum-window-substring  # id : 76"
      ],
      "metadata": {
        "id": "zJG2Y4IxSV6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_retriever.invoke(\"Please write example code to solve the problem of Integer to Roman\")\n",
        "# slug: integer-to-romam  # id: 12, 13, 78, 7"
      ],
      "metadata": {
        "id": "2qiJRhtwSR7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_retriever.invoke(\"i dont know about word break problem. please give me an approch about this problem in python.\")"
      ],
      "metadata": {
        "id": "C-Td8p3jRQ4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_retriever.invoke(\"Please write example code to solve the problem of Find the Index of the First Occurrence in a String\")\n",
        "# slug:"
      ],
      "metadata": {
        "id": "VXewPg8tRSG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever Gemma-2b-it"
      ],
      "metadata": {
        "id": "yNWHYysURKfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain import hub\n",
        "import faiss\n",
        "\n",
        "# 환경변수 설정\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'your token'\n",
        "os.environ['OPENAI_API_KEY'] = 'your token'\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"your token\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"your token\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "\n",
        "# 모델 및 토크나이저 로드\n",
        "access_token = 'your token'\n",
        "\n",
        "\n",
        "# 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\n",
        "# 데이터 로드\n",
        "dataset_name = \"greengerong/leetcode\"\n",
        "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column=\"content\")\n",
        "docs = loader.load()\n",
        "\n",
        "# 문서 결합 및 처리\n",
        "for doc in docs:\n",
        "    title = doc.metadata.get('title', 'No Title')\n",
        "    slug = doc.metadata.get('slug', 'No Slug')\n",
        "    id = doc.metadata.get('id', 'No ID')\n",
        "    doc.page_content = f\"Title: {title}\\n\\nID: LeetCode_number_{id}\\n\\nSlug: {slug}\\n\\n{doc.page_content}\"\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=100)\n",
        "split_docs = text_splitter.split_documents(docs)[:10]  # 제한된 수의 문서만 사용\n",
        "\n",
        "# 벡터 스토어 및 검색 엔진 재설정\n",
        "vectorstore = FAISS.from_documents(documents=split_docs, embedding=HuggingFaceBgeEmbeddings())\n",
        "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
        "faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever])\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", token = access_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", token = access_token)\n",
        "\n",
        "def generate_response(query):\n",
        "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in split_docs)\n",
        "    prompt = f\"{query}\\n\\n{formatted_docs}\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=1024)\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=512, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Document Search and Response Generation\")\n",
        "query = st.text_input(\"Enter your query:\")\n",
        "if query:\n",
        "    response = generate_response(query)\n",
        "    st.write(\"Response:\", response)\n"
      ],
      "metadata": {
        "id": "QcGNWgWPRKCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2uvs5QpQ1sRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PAJuDool1xYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b7Jor1ac1zO0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}