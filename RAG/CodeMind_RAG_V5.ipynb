{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPsguCk26qEpX4vSs9OcC/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhyha/CodeMind_RAG/blob/main/RAG/CodeMind_RAG_V5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ë ˆí¬ì§€í† ë¦¬ì—ì„œ .envì™€ requirements.txt contentì— ì—…ë¡œë“œí•˜ì—¬ ë¶™ì—¬ë„£ê¸°\n",
        "# -U update   -q quiet  -r read\n",
        "# env.txt íŒŒì¼ -> .env ë¡œ ë³€ê²½ í›„  ê²½ë¡œ ë¶™ì—¬ë„£ê¸°"
      ],
      "metadata": {
        "id": "X3P9YIkz7kaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q -r /content/requirement.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7CRsSb17Xux",
        "outputId": "57869f88-cc72-453f-e343-6719042bd900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m526.8/526.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m311.5/311.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m222.5/222.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m291.3/291.3 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.1/106.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.2.2 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.2.2 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.2.2 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from dotenv import load_dotenv\n",
        "# import os\n",
        "\n",
        "# # Specify the path to your .env file\n",
        "# env_path = '/content/.env'\n",
        "\n",
        "# # Load the .env file from the specified path\n",
        "# load_dotenv(dotenv_path=env_path)"
      ],
      "metadata": {
        "id": "wEeF9ty7jT5i",
        "outputId": "148abe3e-23bf-4eb5-8964-455f1a4bbb59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eJHnj5W0Mx-"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain_community.document_loaders.hugging_face_dataset import (\n",
        "    HuggingFaceDatasetLoader,\n",
        ")\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.llms import HuggingFaceHub\n",
        "import os\n",
        "\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit"
      ],
      "metadata": {
        "id": "HKHNafjC1eP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# import faiss\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain import hub\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# # í™˜ê²½ë³€ìˆ˜ ì„¤ì •\n",
        "# os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'your token'\n",
        "# os.environ['OPENAI_API_KEY'] = 'your token'\n",
        "\n",
        "# # Langsmith\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = \"your token\"\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = \"streamlit_v2\"\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "\n",
        "# í™˜ê²½ë³€ìˆ˜ ì„¤ì •\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
        "os.environ['OPENAI_API_KEY'] = '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'\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__b34e03adb0cb40e497e967360e32e029\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"streamlit_v2\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "\n",
        "# ë°ì´í„° ë° ì»´í¬ë„ŒíŠ¸ ë¡œë“œë¥¼ ìœ„í•œ í•¨ìˆ˜\n",
        "@st.cache_data\n",
        "def load_components():\n",
        "    # ë°ì´í„° ë¡œë“œ\n",
        "    dataset_name = \"greengerong/leetcode\"\n",
        "    loader = HuggingFaceDatasetLoader(dataset_name, page_content_column=\"content\")\n",
        "    docs = loader.load()\n",
        "\n",
        "    # RAG ì„±ëŠ¥ ê²€ì¦, 100ë²ˆ ì´ìƒì˜ ë²ˆí˜¸ë¥¼ ë„£ì–´ë³´ì\n",
        "    limited_docs = docs[:100]\n",
        "    # Initialize the text splitter with specified chunk size and overlap\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=100)\n",
        "    # Apply the splitter to the reduced list of documents\n",
        "    docs = text_splitter.split_documents(limited_docs)\n",
        "\n",
        "    # 'title', 'slug', 'page_content' ê²°í•©\n",
        "    for doc in docs:\n",
        "        title = doc.metadata.get('title', 'No Title') if 'title' in doc.metadata else 'No Title'\n",
        "        slug = doc.metadata.get('slug', 'No Slug') if 'slug' in doc.metadata else 'No Slug'\n",
        "        id = doc.metadata.get('id', 'No ID') if 'id' in doc.metadata else 'No ID'\n",
        "        page_content = doc.page_content\n",
        "        doc.page_content = f\"Title: {id}. {title}\\n\\nSlug: {id}. {slug}\\n\\n{page_content}\"  # 'page_content' ì†ì„± ìˆ˜ì •\n",
        "\n",
        "    # ë¬¸ì„œ ë¶„í• \n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=100)\n",
        "    split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "    # ë²¡í„° ìŠ¤í† ì–´ ë° ê²€ìƒ‰ ì—”ì§„ ì„¤ì •\n",
        "    vectorstore = FAISS.from_documents(documents=split_docs, embedding=HuggingFaceBgeEmbeddings())\n",
        "    bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
        "    bm25_retriever.k = 1\n",
        "    faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "    ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0, 1])\n",
        "\n",
        "    # return ensemble_retriever, split_docs\n",
        "    return faiss_retriever, split_docs\n",
        "\n",
        "# ensemble_retriever, split_docs = load_components()\n",
        "faiss_retriever, split_docs = load_components()\n",
        "\n",
        "############### llm ëª¨ë¸ê³¼ prompt ì •ì˜\n",
        "\n",
        "class StreamCallback(BaseCallbackHandler):\n",
        "    def on_llm_new_token(self, token: str, **kwargs):\n",
        "        print(token, end=\"\", flush=True)\n",
        "\n",
        "# LLM ì„¤ì •\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, streaming=True, callbacks=[StreamCallback()])\n",
        "\n",
        "prompt = hub.pull(\"nhyha/rag-prompt\")\n",
        "\n",
        "\n",
        "################ ì…ë ¥ë°›ì•„ ì ì ˆí•œ í˜•íƒœì˜ llmëª¨ë¸ì˜ inputìœ¼ë¡œ ë³€í™˜\n",
        "\n",
        "\n",
        "# ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤\n",
        "def format_docs(retrieved_docs, language):\n",
        "    formatted_docs = []\n",
        "    for doc in retrieved_docs:\n",
        "        language_content = doc.metadata.get(language.lower(), f'No {language} code available')\n",
        "        title = doc.metadata.get('title', 'No Title') if 'title' in doc.metadata else 'No Title'\n",
        "        slug = doc.metadata.get('slug', 'No Slug') if 'slug' in doc.metadata else 'No Slug'\n",
        "        id = doc.metadata.get('id', 'No ID') if 'id' in doc.metadata else 'No ID'\n",
        "        # formatted_doc = f\"{doc.page_content}\\n\\nTitle: {id}. {title}\\n\\nSlug: {id}. {slug}\\n\\n{language}:{language_content}\"\n",
        "        formatted_doc = f\"Title: {id}. {title}\\n\\n{language}:{language_content}\"\n",
        "        formatted_docs.append(formatted_doc)\n",
        "    return \"\\n\\n\".join(formatted_docs)\n",
        "\n",
        "\n",
        "st.title(\"CodeMind_RAG\")\n",
        "st.markdown('''ğŸ†**CodeMind ì„ ì„œ**ğŸ†<br>\n",
        "<span style=\"color: black; font-size: 14px;\">ì €ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ ì§€í‚¬ ê²ƒì„ ì„ ì–¸í•©ë‹ˆë‹¤.<br><br>\n",
        "<span style=\"color: black; font-size: 20px;\">#1</span>  ì €ëŠ” ê²°ì½” ì œ ë°ì´í„° ë² ì´ìŠ¤ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë‚´ìš©ì— ëŒ€í•´ì„œ<br>\n",
        "  **ì‚¬ì‹¤ì²˜ëŸ¼ ê¾¸ë©°ë‚´ê±°ë‚˜ ê±°ì§“ ë‹µë³€**í•˜ì§€ ì•Šê² ìŠµë‹ˆë‹¤.<br><br>\n",
        "<span style=\"color: black; font-size: 20px;\">#2</span>  ì €ëŠ” í˜„ì¬ ì œê°€ ê±°ì§“ìœ¼ë¡œ ë‹µë³€í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•˜ê³ ì<br>\n",
        "  **LeetCode 1 ~ 100ë²ˆ ë¬¸ì œë§Œ** DBì— ì €ì¥ë˜ì–´ ìˆëŠ” ìƒíƒœì…ë‹ˆë‹¤.<br><br>\n",
        "  **ë”°ë¼ì„œ 101ë²ˆ ì´ìƒì˜ ë¬¸ì œë¥¼ ë¬¼ì–´ë³´ì‹ ë‹¤ë©´ í˜„ì¬ëŠ” ëª¨ë¥¸ë‹¤ê³  ëŒ€ë‹µí•˜ëŠ” ê²Œ ì •ìƒì…ë‹ˆë‹¤!**<br><br>\n",
        "  ë¬¼ë¡ , **101ë²ˆ ê·¸ ì´ìƒê¹Œì§€ í˜¹ì€ ë˜ ë‹¤ë¥¸ ì‚¬ì´íŠ¸ì˜ ë¬¸ì œê¹Œì§€ë„**<br>\n",
        "  ë°ì´í„° ë² ì´ìŠ¤ì— ì–¼ë§ˆë“ ì§€ ë‚´ìš©ì„ **ì¶”ê°€**í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.<br><br>\n",
        "<span style=\"color: black; font-size: 20px;\">#3</span>  ì œê°€ ë‹µë³€ë“œë¦¬ëŠ” ë‚´ìš©ì€ **ìˆ˜ ë§ì€ ê°œë°œìë¶„ë“¤ê»˜ì„œ ì¶”ì²œ**í•œ<br>\n",
        "  **solution**ì„ í† ëŒ€ë¡œ ì„ ë³„í•˜ì—¬ ìš”ì•½ ì •ë¦¬í•´ë“œë¦¬ê³  ìˆìŠµë‹ˆë‹¤. í™œìš©ì— ì°¸ê³ í•´ì£¼ì„¸ìš”! ê°ì‚¬í•©ë‹ˆë‹¤ğŸ¤—</span>\n",
        "  ''', unsafe_allow_html=True)\n",
        "# query = st.text_input(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:\", key=\"query_input\")\n",
        "query = st.text_area(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:\", key=\"query_input\")\n",
        "type_select = st.selectbox(\"Select the programming language:\", [\"Python\", \"C++\", \"Java\", \"JavaScript\"], key=\"language_select\")\n",
        "\n",
        "if query:\n",
        "    # ë¬¸ì„œ ê²€ìƒ‰ ì‹¤í–‰\n",
        "    # retrieved_docs = ensemble_retriever.get_relevant_documents(query)\n",
        "    retrieved_docs = faiss_retriever.get_relevant_documents(query)\n",
        "\n",
        "    # ë¬¸ì„œ í˜•ì‹ ì •ì˜\n",
        "    if retrieved_docs:\n",
        "        formatted_docs = format_docs(retrieved_docs, type_select)\n",
        "    else:\n",
        "        formatted_docs = \"ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.\"\n",
        "\n",
        "    # Correctly set the input for RunnablePassthrough objects\n",
        "    # Assuming there's a method to set input or modify the chain to accept inputs directly\n",
        "    context_runnable = RunnablePassthrough.assign(context=lambda x: formatted_docs)\n",
        "    question_runnable = RunnablePassthrough.assign(question=lambda x: query)\n",
        "\n",
        "    # Define the chain with the corrected usage of RunnablePassthrough objects\n",
        "    rag_chain = (\n",
        "        context_runnable\n",
        "        | question_runnable\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    # Invoke the chain\n",
        "    response = rag_chain.invoke({\"context\": formatted_docs, \"question\": query})\n",
        "    st.markdown(\"\\n\\nğŸ’¡**CodeMindChat**\\n\\n\")\n",
        "    st.write(response)\n",
        "\n",
        "\n",
        "\n",
        "# =====================================\n",
        "\n",
        "# # def format_docs(docs):\n",
        "# #     formatted_docs = []\n",
        "# #     for doc in docs:\n",
        "# #         title = doc.metadata.get('title', 'No Title') if 'title' in doc.metadata else 'No Title'\n",
        "# #         slug = doc.metadata.get('slug', 'No Slug')  if 'slug' in doc.metadata else 'No Slug'\n",
        "# #         id = doc.metadata.get('id', 'No ID') if 'id' in doc.metadata else 'No ID'\n",
        "# #         py = doc.metadata.get ('python','No Python') if 'python' in doc.metadata else 'No Python'\n",
        "# #         cpp = doc.metadata.get ('cpp','No Cpp') if 'cpp' in doc.metadata else 'No Cpp'\n",
        "# #         java = doc.metadata.get ('java','No Java') if 'java' in doc.metadata else 'No Java'\n",
        "# #         javascript = doc.metadata.get ('javascript','No Javascript') if 'javascript' in doc.metadata else 'No Javascript'\n",
        "# #         page_content = doc.page_content\n",
        "# #         formatted_doc = f\"Title: {id}. {title}\\n\\nSlug: {id}. {slug}\\n\\n{page_content}\"\n",
        "# #         formatted_docs.append(formatted_doc)\n",
        "# #     return \"\\n\\n\".join(formatted_docs)\n",
        "\n",
        "# def format_docs(docs):\n",
        "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# # Streamlit ì¸í„°í˜ì´ìŠ¤\n",
        "# st.title(\"ë¬¸ì„œ ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„±\")\n",
        "# query = st.text_input(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:\")\n",
        "\n",
        "# if query:\n",
        "#     rag_chain = (\n",
        "#         {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "#         | prompt\n",
        "#         | llm\n",
        "#         | StrOutputParser()\n",
        "#     )\n",
        "#     response = rag_chain.invoke(query)\n",
        "#     st.write(\"Response:\\n\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCMZdFPDH3kR",
        "outputId": "d52f6b3f-617b-4caf-81cb-a0dfe69aaeb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2f9zvJxKCwgBCOHmyWDKw9FSRwQ_ufQqGCV7pvwgcKbA8zfQ"
      ],
      "metadata": {
        "id": "PEXjpKdB0wkK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb29fef-ca39-4ece-d449-99c8f5629a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok http --domain=intimate-neutral-skink.ngrok-free.app"
      ],
      "metadata": {
        "id": "SOqrVDKH0w-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a59d13-ab80-4cbc-c840-26e1c4b68719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "http - start an HTTP tunnel\n",
            "\n",
            "USAGE:\n",
            "  ngrok http [address:port | port] [flags]\n",
            "\n",
            "AUTHOR:\n",
            "  ngrok - <support@ngrok.com>\n",
            "\n",
            "COMMANDS: \n",
            "  config          update or migrate ngrok's configuration file\n",
            "  http            start an HTTP tunnel\n",
            "  tcp             start a TCP tunnel\n",
            "  tunnel          start a tunnel for use with a tunnel-group backend\n",
            "\n",
            "EXAMPLES: \n",
            "  ngrok http 80                                                 # secure public URL for port 80 web server\n",
            "  ngrok http --domain baz.ngrok.dev 8080                        # port 8080 available at baz.ngrok.dev\n",
            "  ngrok tcp 22                                                  # tunnel arbitrary TCP traffic to port 22\n",
            "  ngrok http 80 --oauth=google --oauth-allow-email=foo@foo.com  # secure your app with oauth\n",
            "\n",
            "Paid Features: \n",
            "  ngrok http 80 --domain mydomain.com                           # run ngrok with your own custom domain\n",
            "  ngrok http 80 --allow-cidr 2600:8c00::a03c:91ee:fe69:9695/32  # run ngrok with IP policy restrictions\n",
            "  Upgrade your account at https://dashboard.ngrok.com/billing/subscription to access paid features\n",
            "\n",
            "Upgrade your account at https://dashboard.ngrok.com/billing/subscription to access paid features\n",
            "\n",
            "Flags:\n",
            "  -h, --help      help for ngrok\n",
            "\n",
            "Use \"ngrok [command] --help\" for more information about a command.\n",
            "\n",
            "ERROR:  You must specify a single argument: a port or address to tunnel to.\n",
            "ERROR:  You specified 0 arguments: []\n",
            "ERROR:  For example, to expose port 80, run 'ngrok http 80'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# í¬íŠ¸ 8501ì— ëŒ€í•œ ngrok í„°ë„ì„ ì„¤ì •í•©ë‹ˆë‹¤\n",
        "http_tunnel = ngrok.connect(addr=\"8501\", proto=\"http\", bind_tls=True)\n",
        "print('Streamlitì´ ë‹¤ìŒ URLì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤:', http_tunnel.public_url)\n",
        "\n",
        "!streamlit run app.py &>/dev/null&"
      ],
      "metadata": {
        "id": "8W38LAtC0z39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8043ea0-87a0-4ffe-de75-4d43bfa159c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlitì´ ë‹¤ìŒ URLì—ì„œ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤: https://1a41-35-187-250-224.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w_EWe4FSyzrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fRS35QWrDbxK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jIz5oyETje8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=============================="
      ],
      "metadata": {
        "id": "dzJ45ozb9aAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "p2CotEgh_kA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever gpt-3.5-turbo"
      ],
      "metadata": {
        "id": "lqHSvlHR1Vr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain import hub\n",
        "from langchain.callbacks.base import BaseCallbackHandler\n",
        "\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain_openai import ChatOpenAI\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "# # í™˜ê²½ë³€ìˆ˜ ì„¤ì •\n",
        "# os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'your token'\n",
        "# os.environ['OPENAI_API_KEY'] = 'your token'\n",
        "\n",
        "# # Langsmith\n",
        "# os.environ[\"LANGCHAIN_API_KEY\"] = \"your token\"\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = \"streamlit_v2\"\n",
        "# os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "\n",
        "# ë°ì´í„° ë¡œë“œ\n",
        "dataset_name = \"greengerong/leetcode\"\n",
        "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column=\"content\")\n",
        "docs = loader.load()\n",
        "\n",
        "\n",
        "\n",
        "# 'title', 'slug', 'page_content' ê²°í•©\n",
        "for doc in docs:\n",
        "    title = doc.metadata.get('title', 'No Title') if 'title' in doc.metadata else 'No Title'\n",
        "    slug = doc.metadata.get('slug', 'No Slug') if 'slug' in doc.metadata else 'No Slug'\n",
        "    id = doc.metadata.get('id', 'No ID') if 'id' in doc.metadata else 'No ID'\n",
        "    page_content = doc.page_content\n",
        "    doc.page_content = f\"Title: {id}. {title}\\n\\nSlug: {id}. {slug}\\n\\n{page_content}\"  # 'page_content' ì†ì„± ìˆ˜ì •\n",
        "\n",
        "# ë³€ê²½ëœ page_contentë¥¼ ë°˜ì˜í•˜ê¸° ìœ„í•´ ë¬¸ì„œë¥¼ ë‹¤ì‹œ ë¶„í• \n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=100)\n",
        "split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "# ë³€ê²½ëœ ë¬¸ì„œ ë‚´ìš©ìœ¼ë¡œ ë²¡í„° ìŠ¤í† ì–´ ë° ê²€ìƒ‰ ì—”ì§„ ì¬ì„¤ì •\n",
        "vectorstore = FAISS.from_documents(documents=split_docs, embedding=HuggingFaceBgeEmbeddings())\n",
        "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
        "bm25_retriever.k = 6\n",
        "faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 6})\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.1, 0.9])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class StreamCallback(BaseCallbackHandler):\n",
        "    def on_llm_new_token(self, token: str, **kwargs):\n",
        "        print(token, end=\"\", flush=True)\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0,\n",
        "    streaming=True,\n",
        "    callbacks=[StreamCallback()],\n",
        ")\n",
        "\n",
        "def format_docs(docs):\n",
        "    # ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤.\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "prompt = hub.pull(\"nhyha/rag-prompt\")\n"
      ],
      "metadata": {
        "id": "21Zzji601cV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë‹¨ê³„ 8: ì²´ì¸ ìƒì„±(Create Chain)\n",
        "rag_chain = (\n",
        "    {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# question = \"Please write example code to solve the problem of minimum-window-substring.\"\n",
        "# question = \"Please write example code to solve the problem of Integer to Roman\"\n",
        "# question = \"Please write example code to solve the problem of LeetCode number 12.\"\n",
        "\n",
        "# question = \"Please write example code to solve the problem of LeetCode number #28.\"\n",
        "question = \"Please write example code to solve the problem of Find the Index of the First Occurrence in a String\"\n",
        "\n",
        "response = rag_chain.invoke(question)\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "print(f\"Dataset Path: {dataset_name}\")\n",
        "print(f\"ë¬¸ì„œì˜ ìˆ˜: {len(split_docs)}\")\n",
        "print(\"===\" * 20)\n",
        "print(f\"[HUMAN]\\n{question}\\n\")\n",
        "print(f\"[AI]\\n{response}\")"
      ],
      "metadata": {
        "id": "fGPmkXeJE32y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_retriever.invoke(\"Please write example code to solve the problem of minimum-window-substring.\")\n",
        "# slug : minimum-window-substring  # id : 76"
      ],
      "metadata": {
        "id": "zJG2Y4IxSV6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_retriever.invoke(\"Please write example code to solve the problem of Integer to Roman\")\n",
        "# slug: integer-to-romam  # id: 12, 13, 78, 7"
      ],
      "metadata": {
        "id": "2qiJRhtwSR7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_retriever.invoke(\"i dont know about word break problem. please give me an approch about this problem in python.\")"
      ],
      "metadata": {
        "id": "C-Td8p3jRQ4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_retriever.invoke(\"Please write example code to solve the problem of Find the Index of the First Occurrence in a String\")\n",
        "# slug:"
      ],
      "metadata": {
        "id": "VXewPg8tRSG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever Gemma-2b-it"
      ],
      "metadata": {
        "id": "yNWHYysURKfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "from langchain_community.document_loaders import HuggingFaceDatasetLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain import hub\n",
        "import faiss\n",
        "\n",
        "# í™˜ê²½ë³€ìˆ˜ ì„¤ì •\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'your token'\n",
        "os.environ['OPENAI_API_KEY'] = 'your token'\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"your token\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"your token\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "\n",
        "# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "access_token = 'your token'\n",
        "\n",
        "\n",
        "# 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\n",
        "# ë°ì´í„° ë¡œë“œ\n",
        "dataset_name = \"greengerong/leetcode\"\n",
        "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column=\"content\")\n",
        "docs = loader.load()\n",
        "\n",
        "# ë¬¸ì„œ ê²°í•© ë° ì²˜ë¦¬\n",
        "for doc in docs:\n",
        "    title = doc.metadata.get('title', 'No Title')\n",
        "    slug = doc.metadata.get('slug', 'No Slug')\n",
        "    id = doc.metadata.get('id', 'No ID')\n",
        "    doc.page_content = f\"Title: {title}\\n\\nID: LeetCode_number_{id}\\n\\nSlug: {slug}\\n\\n{doc.page_content}\"\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=100)\n",
        "split_docs = text_splitter.split_documents(docs)[:10]  # ì œí•œëœ ìˆ˜ì˜ ë¬¸ì„œë§Œ ì‚¬ìš©\n",
        "\n",
        "# ë²¡í„° ìŠ¤í† ì–´ ë° ê²€ìƒ‰ ì—”ì§„ ì¬ì„¤ì •\n",
        "vectorstore = FAISS.from_documents(documents=split_docs, embedding=HuggingFaceBgeEmbeddings())\n",
        "bm25_retriever = BM25Retriever.from_documents(split_docs)\n",
        "faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever])\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", token = access_token)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", token = access_token)\n",
        "\n",
        "def generate_response(query):\n",
        "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in split_docs)\n",
        "    prompt = f\"{query}\\n\\n{formatted_docs}\"\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=1024)\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=512, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Document Search and Response Generation\")\n",
        "query = st.text_input(\"Enter your query:\")\n",
        "if query:\n",
        "    response = generate_response(query)\n",
        "    st.write(\"Response:\", response)\n"
      ],
      "metadata": {
        "id": "QcGNWgWPRKCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2uvs5QpQ1sRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PAJuDool1xYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b7Jor1ac1zO0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}